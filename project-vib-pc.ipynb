{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12198654,"sourceType":"datasetVersion","datasetId":7684176},{"sourceId":12199539,"sourceType":"datasetVersion","datasetId":7684661}],"dockerImageVersionId":31042,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport numpy as np\nfrom torch.utils.data import Dataset\nimport torch\nfrom scipy.signal import butter, filtfilt\nfrom torch.utils.data import random_split\n\ndef bandpass_filter(signal):\n    fs=100000\n    lowcut=50\n    highcut=10000\n    order=4\n    nyquist = 0.5 * fs\n    b, a = butter(order, [lowcut / nyquist, highcut / nyquist], btype='band')\n    return filtfilt(b, a, signal)\nclass VibrationSignalDataset(Dataset):\n    def __init__(self, root_dir, fs=100000, segment_len=2048):\n        self.X, self.y = [], []\n        self.fs = fs\n        self.segment_len = segment_len\n\n        for label_str in sorted(os.listdir(root_dir)):\n            label_path = os.path.join(root_dir, label_str)\n            if not os.path.isdir(label_path): continue\n            label = int(label_str)  # Assumes folder name = class label\n            for fname in os.listdir(label_path):\n                if not fname.endswith(\".txt\"): continue\n                fpath = os.path.join(label_path, fname)\n                signal =bandpass_filter(np.loadtxt(fpath))\n                # Filter dan segmentasi\n                segments = [\n                    signal[i:i+segment_len]\n                    for i in range(0, len(signal[100:]) - segment_len + 1, segment_len)\n                ]\n                self.X.extend(segments)\n                self.y.extend([label]*len(segments))\n\n        self.X = np.array(self.X)\n        self.y = np.array(self.y)\n\n    def __len__(self): return len(self.X)\n\n    def __getitem__(self, idx):\n        x = self.X[idx]\n        x = (x - x.mean()) / (x.std() + 1e-8)  # Z-score normalization\n        return torch.tensor(x, dtype=torch.float32).unsqueeze(0), torch.tensor(self.y[idx])\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-18T15:14:37.085375Z","iopub.execute_input":"2025-06-18T15:14:37.085824Z","iopub.status.idle":"2025-06-18T15:15:05.125981Z","shell.execute_reply.started":"2025-06-18T15:14:37.085797Z","shell.execute_reply":"2025-06-18T15:15:05.120980Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n\nclass CNN_GRU(nn.Module):\n    def __init__(self, time_steps, num_classes=3):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv1d(1, 16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm1d(16),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),  # (B, 16, L/2)\n\n            nn.Conv1d(16, 32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm1d(32),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),  # (B, 32, L/4)\n\n            nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1),\n            nn.BatchNorm1d(64),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),  # (B, 64, L/8)\n\n            nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1),  # Tambahan\n            nn.BatchNorm1d(128),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),   # (B, 128, L/16)\n\n            nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1),  # Tambahan\n            nn.BatchNorm1d(256),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2),   # (B, 128, L/16)\n\n            nn.Conv1d(256, 512, kernel_size=3, stride=1, padding=1),  # Tambahan\n            nn.BatchNorm1d(512),\n            nn.ReLU(),\n            nn.MaxPool1d(kernel_size=2)   # (B, 128, L/16)\n        )\n\n        # Hitung ulang time steps setelah pooling (4x maxpool => /16)\n        out_time = time_steps // 16\n\n        self.gru = nn.GRU(input_size=512, hidden_size=1024, batch_first=True)\n\n        self.classifier = nn.Sequential(\n            nn.Linear(1024, 32),\n            nn.ReLU(),\n            nn.Dropout(0.5),\n            nn.Linear(32, num_classes)\n        )\n\n    def forward(self, x):\n        # x: [batch, 1, time]\n        x = self.conv(x)               # => [batch, 128, T/16]\n        x = x.permute(0, 2, 1)         # => [batch, seq_len, features]\n        out, _ = self.gru(x)           # => [batch, seq_len, 256]\n        out = out[:, -1, :]            # Ambil output GRU terakhir\n        return self.classifier(out)\n\ndef train_model(model, train_loader, val_loader, device, epochs=100):\n    model.to(device)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n    criterion = torch.nn.CrossEntropyLoss()\n\n    train_losses = []\n    val_losses = []\n    val_accuracies = []\n\n    for epoch in range(epochs):\n        # --- Train ---\n        model.train()\n        running_loss = 0.0\n        for xb, yb in train_loader:\n            xb, yb = xb.to(device), yb.to(device)\n            optimizer.zero_grad()\n            preds = model(xb)\n            loss = criterion(preds, yb)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        train_losses.append(running_loss / len(train_loader))\n\n        # --- Validation ---\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        with torch.no_grad():\n            for xb, yb in val_loader:\n                xb, yb = xb.to(device), yb.to(device)\n                preds = model(xb)\n                loss = criterion(preds, yb)\n                val_loss += loss.item()\n                _, predicted = torch.max(preds, 1)\n                total += yb.size(0)\n                correct += (predicted == yb).sum().item()\n        val_losses.append(val_loss / len(val_loader))\n        val_accuracies.append(correct / total)\n        print(f\"Epoch {epoch+1}/{epochs} | Train Loss: {train_losses[-1]:.4f} | Val Acc: {val_accuracies[-1]*100:.2f}%\")\n\n    return model, train_losses, val_accuracies\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T15:29:53.098385Z","iopub.execute_input":"2025-06-18T15:29:53.098805Z","iopub.status.idle":"2025-06-18T15:29:53.121188Z","shell.execute_reply.started":"2025-06-18T15:29:53.098776Z","shell.execute_reply":"2025-06-18T15:29:53.115445Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from torch.utils.data import DataLoader\n\n# Inisialisasi\ndataset = VibrationSignalDataset(\"/kaggle/input/datasetfix/dataset\", fs=100000, segment_len=2048)\ntrain_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Cek satu batch\nfor xb, yb in train_loader:\n    print(\"Batch X:\", xb.shape)  # [batch, 1, 2048]\n    print(\"Batch y:\", yb)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T15:30:01.803923Z","iopub.execute_input":"2025-06-18T15:30:01.804313Z","iopub.status.idle":"2025-06-18T15:30:03.070887Z","shell.execute_reply.started":"2025-06-18T15:30:01.804287Z","shell.execute_reply":"2025-06-18T15:30:03.067348Z"}},"outputs":[{"name":"stdout","text":"Batch X: torch.Size([32, 1, 2048])\nBatch y: tensor([0, 1, 1, 0, 0, 1, 2, 2, 1, 2, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 2, 0,\n        0, 1, 1, 2, 0, 0, 2, 1])\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(dataset.X, dataset.y, test_size=0.2, stratify=dataset.y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T15:30:12.650319Z","iopub.execute_input":"2025-06-18T15:30:12.650683Z","iopub.status.idle":"2025-06-18T15:30:12.686178Z","shell.execute_reply.started":"2025-06-18T15:30:12.650656Z","shell.execute_reply":"2025-06-18T15:30:12.681253Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"train_size = int(0.8 * len(dataset))\nval_size = len(dataset) - train_size\ntrain_ds, val_ds = random_split(dataset, [train_size, val_size])\n\ntrain_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\nval_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n\n# Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = CNN_GRU(num_classes=3,time_steps=0.01)  # removed time_steps\n\n# Train\nmodel, train_losses, val_losses, val_accuracies = train_model(model, train_loader, val_loader, device)\ntorch.save(model.state_dict(), \"/kaggle/working/model.pth\")\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label=\"Train Loss\")\nplt.title(\"Loss per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n# Plot Akurasi Validasi\nplt.subplot(1, 2, 2)\nplt.plot(val_accuracies, label=\"Val Accuracy\", color='green')\nplt.title(\"Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-18T15:30:15.985421Z","iopub.execute_input":"2025-06-18T15:30:15.985789Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/100 | Train Loss: 0.8675 | Val Loss: 1.0084 | Val Acc: 60.93%\nEpoch 2/100 | Train Loss: 0.7889 | Val Loss: 0.8298 | Val Acc: 60.28%\nEpoch 3/100 | Train Loss: 0.7211 | Val Loss: 0.6705 | Val Acc: 71.47%\nEpoch 4/100 | Train Loss: 0.7120 | Val Loss: 0.6780 | Val Acc: 73.26%\nEpoch 5/100 | Train Loss: 0.6716 | Val Loss: 0.6891 | Val Acc: 69.79%\nEpoch 6/100 | Train Loss: 0.6310 | Val Loss: 0.6372 | Val Acc: 69.54%\nEpoch 7/100 | Train Loss: 0.5883 | Val Loss: 0.5805 | Val Acc: 74.81%\nEpoch 8/100 | Train Loss: 0.5864 | Val Loss: 0.6682 | Val Acc: 73.01%\nEpoch 9/100 | Train Loss: 0.5564 | Val Loss: 0.6008 | Val Acc: 76.35%\nEpoch 10/100 | Train Loss: 0.5534 | Val Loss: 0.5654 | Val Acc: 76.74%\nEpoch 11/100 | Train Loss: 0.5313 | Val Loss: 0.5768 | Val Acc: 77.12%\nEpoch 12/100 | Train Loss: 0.5049 | Val Loss: 0.5995 | Val Acc: 77.51%\nEpoch 13/100 | Train Loss: 0.4673 | Val Loss: 0.8281 | Val Acc: 73.78%\nEpoch 14/100 | Train Loss: 0.4112 | Val Loss: 0.7588 | Val Acc: 70.95%\nEpoch 15/100 | Train Loss: 0.4244 | Val Loss: 0.7046 | Val Acc: 71.98%\nEpoch 16/100 | Train Loss: 0.3582 | Val Loss: 0.7074 | Val Acc: 73.78%\nEpoch 17/100 | Train Loss: 0.3583 | Val Loss: 0.6205 | Val Acc: 77.25%\nEpoch 18/100 | Train Loss: 0.3041 | Val Loss: 0.7620 | Val Acc: 76.22%\nEpoch 19/100 | Train Loss: 0.3029 | Val Loss: 0.8453 | Val Acc: 71.98%\nEpoch 20/100 | Train Loss: 0.2719 | Val Loss: 0.8844 | Val Acc: 75.32%\nEpoch 21/100 | Train Loss: 0.2327 | Val Loss: 0.8423 | Val Acc: 74.55%\nEpoch 22/100 | Train Loss: 0.2492 | Val Loss: 0.6907 | Val Acc: 78.53%\nEpoch 23/100 | Train Loss: 0.2070 | Val Loss: 0.9156 | Val Acc: 72.75%\nEpoch 24/100 | Train Loss: 0.1488 | Val Loss: 1.0248 | Val Acc: 75.58%\nEpoch 25/100 | Train Loss: 0.1834 | Val Loss: 0.7723 | Val Acc: 78.41%\nEpoch 26/100 | Train Loss: 0.1564 | Val Loss: 0.8508 | Val Acc: 78.15%\nEpoch 27/100 | Train Loss: 0.1545 | Val Loss: 1.1332 | Val Acc: 74.94%\nEpoch 28/100 | Train Loss: 0.1703 | Val Loss: 0.7875 | Val Acc: 78.41%\nEpoch 29/100 | Train Loss: 0.1100 | Val Loss: 1.3768 | Val Acc: 72.88%\nEpoch 30/100 | Train Loss: 0.1712 | Val Loss: 1.2378 | Val Acc: 75.71%\nEpoch 31/100 | Train Loss: 0.0868 | Val Loss: 1.0729 | Val Acc: 78.41%\nEpoch 32/100 | Train Loss: 0.0968 | Val Loss: 1.0261 | Val Acc: 77.63%\nEpoch 33/100 | Train Loss: 0.1405 | Val Loss: 0.9057 | Val Acc: 77.51%\nEpoch 34/100 | Train Loss: 0.0718 | Val Loss: 0.9939 | Val Acc: 78.41%\nEpoch 35/100 | Train Loss: 0.0805 | Val Loss: 1.1701 | Val Acc: 78.66%\nEpoch 36/100 | Train Loss: 0.0623 | Val Loss: 1.1800 | Val Acc: 80.21%\nEpoch 37/100 | Train Loss: 0.0781 | Val Loss: 1.0269 | Val Acc: 76.61%\nEpoch 38/100 | Train Loss: 0.0687 | Val Loss: 1.3342 | Val Acc: 76.99%\nEpoch 39/100 | Train Loss: 0.0939 | Val Loss: 2.2547 | Val Acc: 64.65%\nEpoch 40/100 | Train Loss: 0.0634 | Val Loss: 1.3507 | Val Acc: 78.15%\nEpoch 41/100 | Train Loss: 0.0831 | Val Loss: 1.5566 | Val Acc: 76.61%\nEpoch 42/100 | Train Loss: 0.0766 | Val Loss: 1.3500 | Val Acc: 76.86%\nEpoch 43/100 | Train Loss: 0.0563 | Val Loss: 1.0647 | Val Acc: 78.02%\nEpoch 44/100 | Train Loss: 0.0557 | Val Loss: 1.1933 | Val Acc: 79.43%\nEpoch 45/100 | Train Loss: 0.0186 | Val Loss: 1.5098 | Val Acc: 78.28%\nEpoch 46/100 | Train Loss: 0.0193 | Val Loss: 1.8018 | Val Acc: 77.76%\nEpoch 47/100 | Train Loss: 0.0465 | Val Loss: 1.4956 | Val Acc: 74.55%\nEpoch 48/100 | Train Loss: 0.0804 | Val Loss: 1.5421 | Val Acc: 74.81%\nEpoch 49/100 | Train Loss: 0.0587 | Val Loss: 1.2646 | Val Acc: 78.02%\nEpoch 50/100 | Train Loss: 0.1091 | Val Loss: 1.1163 | Val Acc: 78.41%\nEpoch 51/100 | Train Loss: 0.0660 | Val Loss: 2.0574 | Val Acc: 68.12%\nEpoch 52/100 | Train Loss: 0.0405 | Val Loss: 1.3266 | Val Acc: 79.18%\nEpoch 53/100 | Train Loss: 0.0700 | Val Loss: 1.2367 | Val Acc: 79.05%\nEpoch 54/100 | Train Loss: 0.0424 | Val Loss: 1.5243 | Val Acc: 78.79%\nEpoch 55/100 | Train Loss: 0.0290 | Val Loss: 1.3294 | Val Acc: 79.18%\nEpoch 56/100 | Train Loss: 0.0455 | Val Loss: 1.2720 | Val Acc: 79.43%\nEpoch 57/100 | Train Loss: 0.1539 | Val Loss: 1.6722 | Val Acc: 74.55%\nEpoch 58/100 | Train Loss: 0.2053 | Val Loss: 1.0765 | Val Acc: 77.76%\nEpoch 59/100 | Train Loss: 0.0355 | Val Loss: 1.2304 | Val Acc: 80.72%\nEpoch 60/100 | Train Loss: 0.0310 | Val Loss: 1.3929 | Val Acc: 80.21%\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"torch.save(model.state_dict(), \"/kaggle/working/model_euler.pth\")\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(10, 4))\nplt.subplot(1, 2, 1)\nplt.plot(train_losses, label=\"Train Loss\")\n#plt.plot(val_losses, label=\"Val Loss\")\nplt.title(\"Loss per Epoch\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Loss\")\nplt.legend()\n\n# Plot Akurasi Validasi\nplt.subplot(1, 2, 2)\nplt.plot(val_accuracies, label=\"Val Accuracy\", color='green')\nplt.title(\"Validation Accuracy\")\nplt.xlabel(\"Epoch\")\nplt.ylabel(\"Accuracy\")\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}